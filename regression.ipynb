{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# README\n",
    "This simple script does the following:\n",
    "- Reads tweets in from gzip files for each user given the directory containing user json + gzip files (the output files from Habeeb's twitter crawler)\n",
    "- Cleans the tweets: takes out unicode characters (assumed to be emojis) and non-alphabetical tokens\n",
    "- Vectorize tweets using bag-of-words representation (tf/tf-idf)\n",
    "- Performs feature selection by removing all but the highest scoring percentage of features (percentage defined by user)\n",
    "- Performs topic modeling with LDA - included in the script in case it might produce some useful insights\n",
    "- Runs regression on the vectorized tweets and random real-numbered y-labels (this will be replaced the ground truth when it is available to us)\n",
    "- Returns real-valued predictions, correlation coefficients for each feature, mean squared error and variance score\n",
    "\n",
    "Notes:\n",
    "- LDA should only use TF, not TF-IDF\n",
    "\n",
    "To Do:\n",
    "- Take in CSV IDs with their corresponding scores\n",
    "- Compare SelectPercentile(...) and SelectKBest(...)\n",
    "- Add k-fold cross-validation\n",
    "- Add option to save/load ML model\n",
    "    - https://machinelearningmastery.com/save-load-machine-learning-models-python-scikit-learn/\n",
    "- Use and compare the following regression models:\n",
    "    - Linear Regression (current)\n",
    "    - Ridge Regression\n",
    "    - Least Angle Regression\n",
    "    - Bayesian Regression\n",
    "    - Logistic Regression\n",
    "    - SVM Regression\n",
    "    - Nearest Neighbors Regression\n",
    "    - Decision Trees Regression\n",
    "    - Gradient Tree Boosting Regression\n",
    "    - NN Regression (Use code from research with Dr. Jiang)\n",
    "- Compare results of using TF-IDF (use TfidfVectorizer(...) instead of CountVectorizer(...)) for regression instead (currently just TF)\n",
    "- With either TfidfVectorizer(...) and CountVectorizer(...), add params like specifying n-grams \n",
    "- Write all results to a .log or .csv file\n",
    "- Loop over all gzips instead of using just one\n",
    "- Look into adding tweet-based features\n",
    "    - https://stackoverflow.com/questions/29653321/add-column-to-tfidf-matrix (reference the 2nd answer)\n",
    "- In each tweet-json, we have:\n",
    "    - DT created\n",
    "    - friends_count\n",
    "    - followers_count\n",
    "    - favourites_count\n",
    "    - retweet_count\n",
    "    - etc. (open json from any gzip to see what other data was scraped)\n",
    "- Refine token filtering: don't just throw out anything that contains non-alphabetic characters\n",
    "- Keep emojis\n",
    "    - https://stackoverflow.com/questions/43146528/how-to-extract-all-the-emojis-from-text\n",
    "- Write another script to run this automatically with different options (which models, what parameters, etc.)\n",
    "- Display statistics/results graphically\n",
    "- Use the hate-speech-dictionary as a feature\n",
    "- Use the time that tweet is created as a feature\n",
    "- Use the syllable count as a feature\n",
    "    - https://stackoverflow.com/questions/46759492/syllable-count-in-python\n",
    "- Use emojis as a feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User-defined Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the directory containing user tweets .json + .gz files\n",
    "TWEETS_DIRECTORY = \"../users-new/\"\n",
    "\n",
    "# Specify the name of the CSV file containing personality scores for each user\n",
    "TRAIN_LABEL_FILE = \"../train_labels.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Program Begins Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.feature_selection import SelectPercentile, SelectKBest\n",
    "from sklearn.feature_selection import f_regression\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import sys\n",
    "import os\n",
    "import gzip\n",
    "import re\n",
    "import json\n",
    "import string\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function definitions\n",
    "\n",
    "# Print the words in their respective topics\n",
    "def printTopWords(model, featureNames, nTopWords):\n",
    "    for topicIdx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topicIdx\n",
    "        message += \" \".join([featureNames[i] for i in topic.argsort()[:-nTopWords-1:-1]])\n",
    "        print(message)\n",
    "    \n",
    "    print()\n",
    "    \n",
    "# Get tweets from one user\n",
    "def getUserTweetsAndData(userTweetsFile):\n",
    "    data = []\n",
    "    tweets = []\n",
    "\n",
    "    with gzip.open(userTweetsFile,'r') as f:        \n",
    "        for tweetDeetsBinary in f:\n",
    "            # Convert each line (binary) to string\n",
    "            tweetDeetsStr = tweetDeetsBinary.decode('utf-8')\n",
    "\n",
    "            # Generate json objects\n",
    "            tweetDeetsJson = json.loads(tweetDeetsStr)\n",
    "            data.append(tweetDeetsJson)\n",
    "\n",
    "            # https://stackoverflow.com/questions/92438/stripping-non-printable-characters-from-a-string-in-python\n",
    "            filteredTweetText = ''.join(filter(lambda x: x in string.printable, tweetDeetsJson[\"text\"]))\n",
    "\n",
    "            # Only keep alphabetical words\n",
    "            filteredTweetText = ' '.join([word for word in filteredTweetText.split(\" \") if word.isalpha()])\n",
    "            tweets.append(filteredTweetText)\n",
    "    \n",
    "    return data, tweets\n",
    "\n",
    "# Get a user's ID\n",
    "def getUserID(TWEETS_DIRECTORY, userTweetsFile):\n",
    "    pattern = \"(\" + TWEETS_DIRECTORY.replace(\"/\", \"\\/\") + \")(\\d+)(\\.json\\.gz)\"\n",
    "    m = re.match(pattern, userTweetsFile)\n",
    "    \n",
    "    return int(m[2])\n",
    "\n",
    "# Get a user's SDO and RWA scores\n",
    "def getUserScores(TWEETS_DIRECTORY, userTweetsFile, df):\n",
    "    userID = getUserID(TWEETS_DIRECTORY, userTweetsFile)\n",
    "    sdo_score = float(df.loc[userID, \"sdo\"])\n",
    "    rwa_score = float(df.loc[userID, \"rwa\"])\n",
    "    \n",
    "    return sdo_score, rwa_score\n",
    "\n",
    "# Print a user's data\n",
    "def printUserStats(TWEETS_DIRECTORY, userTweetsFile, df):\n",
    "    # Get user personality scores\n",
    "    personalityScores = getUserScores(TWEETS_DIRECTORY, userTweetsFile, df)\n",
    "    sdo_score = personalityScores[0]\n",
    "    rwa_score = personalityScores[1]\n",
    "    \n",
    "    # Print user data\n",
    "    print(\"Current twitter user file:\", userTweetsFile)\n",
    "    print(\"Number of tweets:\", int(df.loc[userID, \"num_tweets\"]))\n",
    "    print(\"Number of nighttime tweets:\", int(df.loc[userID, \"num_night_tweets\"]))\n",
    "    print(\"Number of monday tweets:\", int(df.loc[userID, \"num_mon_tweets\"]))\n",
    "    print(\"Number of tuesday tweets:\", int(df.loc[userID, \"num_tue_tweets\"]))\n",
    "    print(\"Number of wednesday tweets:\", int(df.loc[userID, \"num_wed_tweets\"]))\n",
    "    print(\"Number of thursday tweets:\", int(df.loc[userID, \"num_thu_tweets\"]))\n",
    "    print(\"Number of friday tweets:\", int(df.loc[userID, \"num_fri_tweets\"]))\n",
    "    print(\"Number of saturday tweets:\", int(df.loc[userID, \"num_sat_tweets\"]))\n",
    "    print(\"Number of sunday tweets:\", int(df.loc[userID, \"num_sun_tweets\"]))\n",
    "    print(\"Number of favorites:\", int(df.loc[userID, \"num_favorited\"]))\n",
    "    print(\"Number of retweets:\", int(df.loc[userID, \"num_retweeted\"]))\n",
    "    print(\"Number of followers:\", int(df.loc[userID, \"num_followers\"]))\n",
    "    print(\"SDO Score:\", sdo_score)\n",
    "    print(\"RWA Score:\", rwa_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the gzip files\n",
    "gzipFiles = [TWEETS_DIRECTORY + objname for objname in os.listdir(TWEETS_DIRECTORY) if re.search(r\".+\\.gz$\", objname)]\n",
    "for gzfilename in gzipFiles:\n",
    "    print(gzfilename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read file containing training labels into a dataframe\n",
    "df = pd.read_csv(TRAIN_LABEL_FILE)\n",
    "print(df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify dataframe to hold features for each user\n",
    "featureNames = [\n",
    "    \"num_tweets\",\n",
    "    \"avg_read_score\",\n",
    "    \"num_night_tweets\",\n",
    "    \"racist_score\",\n",
    "    \"sexist_score\",\n",
    "    \"relig_score\",\n",
    "    \"num_emojis\",\n",
    "    \"num_hashtags\",\n",
    "    \"num_retweeted\",\n",
    "    \"num_reshares\",\n",
    "    \"num_mon_tweets\",\n",
    "    \"num_tue_tweets\",\n",
    "    \"num_wed_tweets\",\n",
    "    \"num_thu_tweets\",\n",
    "    \"num_fri_tweets\",\n",
    "    \"num_sat_tweets\",\n",
    "    \"num_sun_tweets\",\n",
    "    \"num_pos_tweets\",\n",
    "    \"num_neut_tweets\",\n",
    "    \"avg_neg_score\",\n",
    "    \"num_links\",\n",
    "    \"num_followers\",\n",
    "    \"num_favorited\",\n",
    "    \"num_pics\",\n",
    "    \"num_linked_pics\"\n",
    "]\n",
    "\n",
    "for featName in featureNames:\n",
    "    df[featName] = np.zeros(df.shape[0])\n",
    "df = df.set_index(\"user_id\")\n",
    "\n",
    "print(df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for userTweetsFile in gzipFiles:\n",
    "    # Get data and tweets\n",
    "    userID = float(getUserID(TWEETS_DIRECTORY, userTweetsFile))\n",
    "    dataAndTweets = getUserTweetsAndData(userTweetsFile)\n",
    "    data = dataAndTweets[0]\n",
    "    tweets = dataAndTweets[1]\n",
    "    \n",
    "    for i in range(len(dataAndTweets[0])):\n",
    "        # Get the average readability score\n",
    "        \n",
    "        # Get the time tweet created\n",
    "        srch = re.search(\"(\\d{2}):\\d{2}:\\d{2}\", data[i][\"created_at\"])\n",
    "        time = srch[0]\n",
    "        hour = srch[1]\n",
    "        if int(hour) > 18:\n",
    "            df.loc[userID, \"num_night_tweets\"] += 1\n",
    "        \n",
    "        # Get the level of racism\n",
    "        \n",
    "        # Get the level of sexism\n",
    "        \n",
    "        # Get the level of religiousness\n",
    "        \n",
    "        # Get whether emojis were used/freq. of emojis\n",
    "        \n",
    "        # Get the number of hashtags\n",
    "        \n",
    "        # Get the number of reposts\n",
    "        \n",
    "        # Get the day of the week tweeted\n",
    "        srch = re.search(\"(\\w{3})\", data[i][\"created_at\"])\n",
    "        day = str(srch[1])\n",
    "        if day == \"Mon\":\n",
    "            df.loc[userID, \"num_mon_tweets\"] += 1\n",
    "        elif day == \"Tue\":\n",
    "            df.loc[userID, \"num_tue_tweets\"] += 1\n",
    "        elif day == \"Wed\":\n",
    "            df.loc[userID, \"num_wed_tweets\"] += 1\n",
    "        elif day == \"Thu\":\n",
    "            df.loc[userID, \"num_thu_tweets\"] += 1\n",
    "        elif day == \"Fri\":\n",
    "            df.loc[userID, \"num_fri_tweets\"] += 1\n",
    "        elif day == \"Sat\":\n",
    "            df.loc[userID, \"num_sat_tweets\"] += 1\n",
    "        elif day == \"Sun\":\n",
    "            df.loc[userID, \"num_sun_tweets\"] += 1\n",
    "\n",
    "        # Get the sentiment of the tweet\n",
    "        \n",
    "        # Get whether a link was present\n",
    "        \n",
    "        # Get the number favorited\n",
    "        df.loc[userID, \"num_favorited\"] += data[i][\"favorite_count\"]\n",
    "        \n",
    "        # Get the number retweeted\n",
    "        df.loc[userID, \"num_retweeted\"] += data[i][\"retweet_count\"]\n",
    "        \n",
    "        # Get the number of pictures\n",
    "        \n",
    "        # Get the number of linked pictures\n",
    "    \n",
    "    # Get the number of tweets total\n",
    "    df.loc[userID, \"num_tweets\"] = len(data)\n",
    "    \n",
    "    # Get the number of follows\n",
    "    df.loc[userID, \"num_followers\"] = data[i][\"user\"][\"followers_count\"]\n",
    "    \n",
    "    # Print all a user's data\n",
    "    printUserStats(TWEETS_DIRECTORY, userTweetsFile, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get complete rows of data from DF and convert to np_array\n",
    "colNames = [\"user_id\", \"sdo\", \"rwa\"] + featureNames\n",
    "newDF = pd.DataFrame(columns=colNames)\n",
    "newDF = newDF.set_index(\"user_id\")\n",
    "\n",
    "for userTweetsFile in gzipFiles:\n",
    "    userID = float(getUserID(TWEETS_DIRECTORY, userTweetsFile))\n",
    "    newDF = newDF.append(df.loc[userID])\n",
    "    \n",
    "npDF = newDF.values\n",
    "print(npDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training/testing sets\n",
    "trainSize = int(.75*npDF.shape[0])\n",
    "Xtrain = npDF[:trainSize, 2:]\n",
    "ytrain = npDF[:trainSize, :2]\n",
    "Xtest = npDF[trainSize:, 2:]\n",
    "ytest = npDF[trainSize:, :2]\n",
    "\n",
    "print(Xtrain.shape)\n",
    "print(ytrain.shape)\n",
    "print(Xtest.shape)\n",
    "print(ytest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Linear Regression\n",
    "regr = linear_model.LinearRegression()\n",
    "regr.fit(Xtrain, ytrain)\n",
    "yPred = regr.predict(Xtest)\n",
    "print(yPred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Coefficients: \\n', regr.coef_)\n",
    "print(\"Mean squared error: %.2f\" % mean_squared_error(ytest, yPred))\n",
    "print('Variance score: %.2f' % r2_score(ytest, yPred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Feature extraction\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "tf = vectorizer.fit_transform(tweets)\n",
    "print(\"tf shape:\", tf.shape)\n",
    "\n",
    "y = np.empty(tf.shape[0])\n",
    "\n",
    "# Give each tweet the personality score of the user\n",
    "if TRAIN_LABEL_FILE == \"\":\n",
    "    # Randomly fill score if none provided\n",
    "    #y.fill(np.random.uniform(size=1)[0])\n",
    "    y = np.random.uniform(size=tf.shape[0])\n",
    "    print(y[0])\n",
    "else:\n",
    "    #y.fill(float(sdo_score))\n",
    "    y = np.random.uniform(size=tf.shape[0])\n",
    "    print(y[0])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature selection\n",
    "#tfNew = SelectKBest(f_regression, k=10).fit_transform(tf, y)\n",
    "selPercent = SelectPercentile(f_regression, percentile=10)\n",
    "tfNew = selPercent.fit_transform(tf, y)\n",
    "print(\"tfNew shape:\", tfNew.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run LDA with original tf\n",
    "lda = LatentDirichletAllocation(n_components=10, max_iter=5,\n",
    "                                learning_method='online',\n",
    "                                learning_offset=50.,\n",
    "                                random_state=0)\n",
    "\n",
    "lda.fit(tf)\n",
    "print(\"Topics in LDA model:\")\n",
    "tfFeatureNames = vectorizer.get_feature_names()\n",
    "printTopWords(lda, tfFeatureNames, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run LDA with selected tf\n",
    "lda2 = LatentDirichletAllocation(n_components=10, max_iter=5,\n",
    "                                learning_method='online',\n",
    "                                learning_offset=50.,\n",
    "                                random_state=0)\n",
    "\n",
    "lda2.fit(tfNew)\n",
    "print(\"Topics in new LDA model:\")\n",
    "tfFeatureNames = vectorizer.get_feature_names()\n",
    "retained = selPercent.get_support(True)\n",
    "newFeatureNames = []\n",
    "\n",
    "# Get the list of selected feature names \n",
    "for idx in retained:\n",
    "    newFeatureNames.append(tfFeatureNames[idx])\n",
    "\n",
    "printTopWords(lda2, newFeatureNames, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
